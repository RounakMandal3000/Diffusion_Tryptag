{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "I3DjmVErVNeN"
   },
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models, transforms\n",
    "from torchvision.utils import save_image, make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation, PillowWriter\n",
    "import numpy as np\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "RXh5WJzawU-h"
   },
   "outputs": [],
   "source": [
    "class ResidualConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, is_res: bool = False) -> None:\n",
    "        super().__init__()\n",
    "        self.same_channels = in_channels == out_channels\n",
    "        self.is_res = is_res\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1, 1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.is_res:\n",
    "            x1 = self.conv1(x)\n",
    "            x2 = self.conv2(x1)\n",
    "            if self.same_channels:\n",
    "                out = x + x2\n",
    "            else:\n",
    "                shortcut = nn.Conv2d(x.shape[1], x2.shape[1], kernel_size=1, stride=1, padding=0).to(x.device)\n",
    "                out = shortcut(x) + x2\n",
    "            return out / 1.414\n",
    "        else:\n",
    "            x1 = self.conv1(x)\n",
    "            x2 = self.conv2(x1)\n",
    "            return x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "QlBlPqCSWBvx"
   },
   "outputs": [],
   "source": [
    "class UnetDown(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UnetDown, self).__init__()\n",
    "        layers = [ResidualConvBlock(in_channels, out_channels), nn.MaxPool2d(2)]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class UnetUp(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UnetUp, self).__init__()\n",
    "        layers = [\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, 2, 2),\n",
    "            ResidualConvBlock(out_channels, out_channels),\n",
    "            ResidualConvBlock(out_channels, out_channels),\n",
    "        ]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        return self.model(torch.cat((x, skip), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "EPD-q_CQlq-9"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels=3, in_dim=32):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.in_dim = in_dim\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels, 3, 1, 1),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_channels=3, in_dim=16):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.in_dim = in_dim\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels, 3, 1, 1),\n",
    "            nn.ConvTranspose2d(in_channels, in_channels, 2, 2),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "yeTlCQ6JWB1-"
   },
   "outputs": [],
   "source": [
    "\n",
    "class EmbedFC(nn.Module):\n",
    "    def __init__(self, input_dim , emb_dim):\n",
    "        super(EmbedFC, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        layers = [\n",
    "            nn.Linear(input_dim, emb_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(emb_dim, emb_dim),\n",
    "        ]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.input_dim).to(device)\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextUnet(nn.Module):\n",
    "    def __init__(self, in_channels, n_feat=256, n_cfeat=10):\n",
    "        super(ContextUnet, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.n_feat = n_feat\n",
    "        self.n_cfeat = n_cfeat\n",
    "        self.init_conv = ResidualConvBlock(in_channels, n_feat, is_res=True)\n",
    "        self.down1 = UnetDown(n_feat, n_feat)\n",
    "        self.down2 = UnetDown(n_feat, 2 * n_feat)\n",
    "        self.to_vec = nn.Sequential(nn.AvgPool2d((4)), nn.GELU())\n",
    "        self.timeembed1 = EmbedFC(1, 2*n_feat)\n",
    "        self.timeembed2 = EmbedFC(1, 1*n_feat)\n",
    "        self.contextembed1 = EmbedFC(n_cfeat, 2*n_feat)\n",
    "        self.contextembed2 = EmbedFC(n_cfeat, 1*n_feat)\n",
    "        self.up0 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(2 * n_feat, 2 * n_feat, 4, 4), \n",
    "            nn.GroupNorm(8, 2 * n_feat),                        \n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.up1 = UnetUp(4 * n_feat, n_feat)\n",
    "        self.up2 = UnetUp(2 * n_feat, n_feat)\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Conv2d(2 * n_feat, n_feat, 3, 1, 1),\n",
    "            nn.GroupNorm(8, n_feat),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n_feat, self.in_channels, 3, 1, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t, c=None):\n",
    "        x = self.init_conv(x)\n",
    "        down1 = self.down1(x)\n",
    "        down2 = self.down2(down1)\n",
    "        hiddenvec = self.to_vec(down2)\n",
    "        if c is None:\n",
    "            c = torch.zeros(x.shape[0], self.n_cfeat).to(x)\n",
    "        cemb1 = self.contextembed1(c).view(-1, self.n_feat * 2, 1, 1)     # (batch, 2*n_feat, 1,1)\n",
    "        temb1 = self.timeembed1(t).view(-1, self.n_feat * 2, 1, 1)\n",
    "        cemb2 = self.contextembed2(c).view(-1, self.n_feat, 1, 1)\n",
    "        temb2 = self.timeembed2(t).view(-1, self.n_feat, 1, 1)\n",
    "        up1 = self.up0(hiddenvec)\n",
    "        up2 = self.up1(cemb1*up1 + temb1, down2)\n",
    "        up3 = self.up2(cemb2*up2 + temb2, down1)\n",
    "        out = self.out(torch.cat((up3, x), 1))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "4F88l1xfV3AF"
   },
   "outputs": [],
   "source": [
    "class ContextUnet(nn.Module):\n",
    "    def __init__(self, in_channels, n_feat=256, n_cfeat=10):\n",
    "        super(ContextUnet, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.n_feat = n_feat\n",
    "        self.n_cfeat = n_cfeat\n",
    "        self.init_conv = ResidualConvBlock(in_channels, n_feat, is_res=True)\n",
    "        self.down1 = UnetDown(n_feat, n_feat)\n",
    "        self.down2 = UnetDown(n_feat, 2 * n_feat)\n",
    "        self.down3 = UnetDown(2 * n_feat, 4 * n_feat)\n",
    "        self.to_vec = nn.Sequential(nn.AvgPool2d((2)), nn.GELU())\n",
    "        self.timeembed1 = EmbedFC(1, 4*n_feat)\n",
    "        self.timeembed2 = EmbedFC(1, 2*n_feat)\n",
    "        self.timeembed3 = EmbedFC(1, 1*n_feat)\n",
    "        self.contextembed1 = EmbedFC(n_cfeat, 4*n_feat)\n",
    "        self.contextembed2 = EmbedFC(n_cfeat, 2*n_feat)\n",
    "        self.contextembed3 = EmbedFC(n_cfeat, 1*n_feat)\n",
    "        self.up0 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(4 * n_feat, 4 * n_feat, 2, 2),\n",
    "            nn.GroupNorm(8, 4 * n_feat),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.up1 = UnetUp(8 * n_feat, 2 * n_feat)\n",
    "        self.up2 = UnetUp(4 * n_feat, n_feat)\n",
    "        self.up3 = UnetUp(2 * n_feat, n_feat)\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Conv2d(2 * n_feat, n_feat, 3, 1, 1),\n",
    "            nn.GroupNorm(8, n_feat),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n_feat, self.in_channels, 3, 1, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t, c=None):\n",
    "        x = self.init_conv(x)\n",
    "        down1 = self.down1(x)\n",
    "        down2 = self.down2(down1)\n",
    "        down3 = self.down3(down2)\n",
    "        hiddenvec = self.to_vec(down3)\n",
    "        if c is None:\n",
    "            c = torch.zeros(x.shape[0], self.n_cfeat).to(x)\n",
    "        cemb1 = self.contextembed1(c).view(-1, self.n_feat * 4, 1, 1)\n",
    "        temb1 = self.timeembed1(t).view(-1, self.n_feat * 4, 1, 1)\n",
    "        cemb2 = self.contextembed2(c).view(-1, self.n_feat * 2, 1, 1)     # (batch, 2*n_feat, 1,1)\n",
    "        temb2 = self.timeembed2(t).view(-1, self.n_feat * 2, 1, 1)\n",
    "        cemb3 = self.contextembed3(c).view(-1, self.n_feat, 1, 1)\n",
    "        temb3 = self.timeembed3(t).view(-1, self.n_feat, 1, 1)\n",
    "\n",
    "        up1 = self.up0(hiddenvec)\n",
    "        up2 = self.up1(cemb1*up1 + temb1, down3)\n",
    "        up3 = self.up2(cemb2*up2 + temb2, down2)\n",
    "        up4 = self.up3(cemb3*up3 + temb3, down1)\n",
    "        out = self.out(torch.cat((up4, x), 1))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "C-n6OkA-wREr",
    "outputId": "f2eba104-819d-4caf-fbf0-8541343597f3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20936601600000002"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated(torch.device(\"cuda\")) * 1e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "P0abri9TWKUA"
   },
   "outputs": [],
   "source": [
    "timesteps = 500\n",
    "beta1 = 1e-4\n",
    "beta2 = 0.02\n",
    "\n",
    "\n",
    "device=torch.device(\"cuda\")\n",
    "n_feat = 256\n",
    "n_cfeat = 10\n",
    "height = 32\n",
    "save_dir = './weights/'\n",
    "\n",
    "batch_size = 100\n",
    "n_epoch = 2000\n",
    "lrate=1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "r8ZmpRZPwREs",
    "outputId": "25ea805b-102f-4a07-84b2-b1cd9718ef75"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.211463168"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated(torch.device(\"cuda\")) * 1e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MdHawegkWMAg"
   },
   "outputs": [],
   "source": [
    "b_t = (beta2 - beta1) * torch.linspace(0, 1, timesteps + 1) + beta1\n",
    "a_t = 1 - b_t\n",
    "ab_t = torch.cumsum(a_t.log(), dim=0).exp()\n",
    "ab_t[0] = 1\n",
    "b_t = b_t.to(device)\n",
    "a_t  = a_t.to(b_t)\n",
    "ab_t = ab_t.to(a_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "3-nbFTpGlq_L"
   },
   "outputs": [],
   "source": [
    "s=0.008\n",
    "f = (torch.cos((torch.linspace(0, timesteps, timesteps + 1)/timesteps+s)/(1+s) * torch.asin(torch.tensor(1))))**2\n",
    "ab_t = f/(torch.cos((s)/(1+s) * torch.asin(torch.tensor(1))))**2\n",
    "b_t = []\n",
    "for i in range(1, timesteps):\n",
    "    b_t.append(1-ab_t[i]/ab_t[i-1])\n",
    "b_t = torch.tensor(b_t)\n",
    "a_t = 1 - b_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "u0967h0bwREs",
    "outputId": "d761e8ab-d801-4dc6-ff2f-abea6a3c9cd0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.211463168"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated(torch.device(\"cuda\")) * 1e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "SOu1LS2AWOov"
   },
   "outputs": [],
   "source": [
    "nn_model = ContextUnet(in_channels=3, n_feat=n_feat, n_cfeat=n_cfeat).to(device)\n",
    "optim = torch.optim.Adam(nn_model.parameters(), lr=lrate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FhvGmM2wXQ4s",
    "outputId": "5953731b-0a85-40f6-db2e-b794a44c80c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import MNIST, CIFAR10\n",
    "tf = transforms.Compose([transforms.ToTensor()])\n",
    "dataset = CIFAR10(\"./data\", train=True, download=True, transform=tf)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "optim = torch.optim.Adam(nn_model.parameters(), lr=lrate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "YTGTJssswREu",
    "outputId": "d7174e54-e07f-4532-ce7a-44ce7ef551aa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20831744000000002"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated(torch.device(\"cuda\")) * 1e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "hWPgKPrOXdmg"
   },
   "outputs": [],
   "source": [
    "# helper function: perturbs an image to a specified noise level\n",
    "def perturb_input(x, t, noise, device=torch.device(\"cuda\")):\n",
    "    final = ab_t.sqrt()[t, None, None, None] * x + (1 - ab_t[t, None, None, None]) * noise\n",
    "    final = final.to(device)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "_qLq6GEKWVzt"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_ddpm_context(n_sample, context, save_rate=20):\n",
    "    # x_T ~ N(0, 1), sample initial noise\n",
    "    samples = torch.randn(n_sample, 3, height, height).to(device)\n",
    "\n",
    "    # array to keep track of generated steps for plotting\n",
    "    intermediate = []\n",
    "    for i in range(timesteps, 0, -1):\n",
    "        # reshape time tensor\n",
    "        t = torch.tensor([i / timesteps])[:, None, None, None].to(device)\n",
    "\n",
    "        # sample some random noise to inject back in. For i = 1, don't add back in noise\n",
    "        z = torch.randn_like(samples) if i > 1 else 0\n",
    "\n",
    "        eps = nn_model(samples, t, c=context)    # predict noise e_(x_t,t, ctx)\n",
    "        #eps = Decoder(3, 3)\n",
    "        samples = denoise_add_noise(samples, i, eps, z)\n",
    "        if i % save_rate==0 or i==timesteps or i<8:\n",
    "            intermediate.append(samples.detach().cpu().numpy())\n",
    "\n",
    "    intermediate = np.stack(intermediate)\n",
    "    return samples, intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "DWbssCNVWSnB"
   },
   "outputs": [],
   "source": [
    "# helper function; removes the predicted noise (but adds some noise back in to avoid collapse)\n",
    "def denoise_add_noise(x, t, pred_noise, z=None):\n",
    "    if z is None:\n",
    "        z = torch.randn_like(x)\n",
    "    noise = b_t.sqrt()[t] * z\n",
    "    mean = (x - pred_noise * ((1 - a_t[t]) / (1 - ab_t[t]).sqrt())) / a_t[t].sqrt()\n",
    "    return mean + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "IP9dpR8zwREu"
   },
   "outputs": [],
   "source": [
    "# define sampling function for DDIM\n",
    "# removes the noise using ddim\n",
    "def denoise_ddim(x, t, t_prev, pred_noise):\n",
    "    ab = ab_t[t]\n",
    "    ab_prev = ab_t[t_prev]\n",
    "\n",
    "    x0_pred = ab_prev.sqrt() / ab.sqrt() * (x - (1 - ab).sqrt() * pred_noise)\n",
    "    dir_xt = (1 - ab_prev).sqrt() * pred_noise\n",
    "\n",
    "    return x0_pred + dir_xt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "dgmjUYpLwREu"
   },
   "outputs": [],
   "source": [
    "# fast sampling algorithm with context\n",
    "@torch.no_grad()\n",
    "def sample_ddim_context(n_sample, context, n=20):\n",
    "    # x_T ~ N(0, 1), sample initial noise\n",
    "    samples = torch.randn(n_sample, 3, height, height).to(device)\n",
    "\n",
    "    # array to keep track of generated steps for plotting\n",
    "    intermediate = []\n",
    "    step_size = timesteps // n\n",
    "    for i in range(timesteps, 0, -step_size):\n",
    "\n",
    "        # reshape time tensor\n",
    "        t = torch.tensor([i / timesteps])[:, None, None, None].to(device)\n",
    "\n",
    "        eps = nn_model(samples, t, c=context)    # predict noise e_(x_t,t)\n",
    "        samples = denoise_ddim(samples, i, i - step_size, eps)\n",
    "        intermediate.append(samples.detach().cpu().numpy())\n",
    "\n",
    "    intermediate = np.stack(intermediate)\n",
    "    return samples, intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "jQNyQpQ9oHRR"
   },
   "outputs": [],
   "source": [
    "def make_encoding(x, n_samples=batch_size, n_classes=n_cfeat):\n",
    "    encode = [\n",
    "    [1,0,0,0,0,0,0,0,0,0],\n",
    "    [0,1,0,0,0,0,0,0,0,0],\n",
    "    [0,0,1,0,0,0,0,0,0,0],\n",
    "    [0,0,0,1,0,0,0,0,0,0],\n",
    "    [0,0,0,0,1,0,0,0,0,0],\n",
    "    [0,0,0,0,0,1,0,0,0,0],\n",
    "    [0,0,0,0,0,0,1,0,0,0],\n",
    "    [0,0,0,0,0,0,0,1,0,0],\n",
    "    [0,0,0,0,0,0,0,0,1,0],\n",
    "    [0,0,0,0,0,0,0,0,0,1]\n",
    "    ]\n",
    "    final = np.zeros((n_samples, n_classes))\n",
    "    for i in range(n_samples):\n",
    "        final[i] = encode[x[i].int()]\n",
    "    final = torch.tensor(final).float().to(device)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "-8zQfjIZWYLf"
   },
   "outputs": [],
   "source": [
    "def show_images(imgs, nrow=2):\n",
    "    _, axs = plt.subplots(nrow, imgs.shape[0] // nrow, figsize=(4,2 ))\n",
    "    axs = axs.flatten()\n",
    "    for img, ax in zip(imgs, axs):\n",
    "        img = (img.permute(1, 2, 0).clip(-1, 1).detach().cpu().numpy() + 1) / 2\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.imshow(img, cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "OEF3LzgMwREv",
    "outputId": "66fb577c-e19f-4fcc-bdf7-936c384f8e34"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2122496"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated(torch.device(\"cuda\")) * 1e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 415
    },
    "id": "e2jSpotiWQJm",
    "outputId": "20745c80-3de3-4729-894e-5f8a72ac7d77"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "nn_model.train()\n",
    "loss_values_1=[]\n",
    "save_dir = \"http://localhost:8888/tree/My_folder\"\n",
    "for ep in range(n_epoch):\n",
    "    print(\"allocated mem -\", torch.cuda.memory_allocated(torch.device(\"cuda\")) * 1e-9)\n",
    "    print(\"epoch number: \", ep)\n",
    "    optim.param_groups[0]['lr'] = lrate*(1-0.75*ep/n_epoch)\n",
    "    pbar = tqdm(dataloader, mininterval = 2)\n",
    "\n",
    "    for x, c in pbar:\n",
    "        optim.zero_grad()\n",
    "        x=x.to(device)\n",
    "        c=c.to(device)\n",
    "        c=make_encoding(c)\n",
    "        #x=Encoder(torch.cuda.FloatTensor(3), torch.cuda.FloatTensor(3))(x_1)\n",
    "    #print(c.shape)\n",
    "        context_mask = torch.bernoulli(torch.zeros(c.shape[0]) + 0.7).to(device)\n",
    "        c = c * context_mask.unsqueeze(-1)\n",
    "        #print(c.shape)\n",
    "        noise = torch.randn_like(x)\n",
    "        t = torch.randint(1, timesteps + 1, (x.shape[0],))\n",
    "        x_pert = perturb_input(x, t, noise).to(device)\n",
    "        pred_noise = nn_model(x_pert, t/timesteps, c)\n",
    "        loss = F.mse_loss(pred_noise, noise)\n",
    "        loss_values_1.append(loss.item())\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    if(ep%4==0):\n",
    "        torch.save(model.state_dict(), save_dir, f\"model_{ep}.pth\")\n",
    "        print(\"Saved model at \" + save_dir + f\"model_{ep}.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "9deNsl98wREv"
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 500 is out of bounds for dimension 0 with size 499",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[90], line 15\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# user defined context\u001b[39;00m\n\u001b[1;32m      2\u001b[0m ctx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# hero, non-hero, food, spell, side-facing\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     [\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     [\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     14\u001b[0m ])\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 15\u001b[0m samples, _ \u001b[38;5;241m=\u001b[39m \u001b[43msample_ddpm_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m show_images(samples)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[82], line 17\u001b[0m, in \u001b[0;36msample_ddpm_context\u001b[0;34m(n_sample, context, save_rate)\u001b[0m\n\u001b[1;32m     15\u001b[0m eps \u001b[38;5;241m=\u001b[39m nn_model(samples, t, c\u001b[38;5;241m=\u001b[39mcontext)    \u001b[38;5;66;03m# predict noise e_(x_t,t, ctx)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#eps = Decoder(3, 3)\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m samples \u001b[38;5;241m=\u001b[39m \u001b[43mdenoise_add_noise\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m save_rate\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m i\u001b[38;5;241m==\u001b[39mtimesteps \u001b[38;5;129;01mor\u001b[39;00m i\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m8\u001b[39m:\n\u001b[1;32m     19\u001b[0m     intermediate\u001b[38;5;241m.\u001b[39mappend(samples\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "Cell \u001b[0;32mIn[83], line 5\u001b[0m, in \u001b[0;36mdenoise_add_noise\u001b[0;34m(x, t, pred_noise, z)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m z \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m      4\u001b[0m     z \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn_like(x)\n\u001b[0;32m----> 5\u001b[0m noise \u001b[38;5;241m=\u001b[39m \u001b[43mb_t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m*\u001b[39m z\n\u001b[1;32m      6\u001b[0m mean \u001b[38;5;241m=\u001b[39m (x \u001b[38;5;241m-\u001b[39m pred_noise \u001b[38;5;241m*\u001b[39m ((\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m a_t[t]) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m ab_t[t])\u001b[38;5;241m.\u001b[39msqrt())) \u001b[38;5;241m/\u001b[39m a_t[t]\u001b[38;5;241m.\u001b[39msqrt()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mean \u001b[38;5;241m+\u001b[39m noise\n",
      "\u001b[0;31mIndexError\u001b[0m: index 500 is out of bounds for dimension 0 with size 499"
     ]
    }
   ],
   "source": [
    "# user defined context\n",
    "ctx = torch.tensor([\n",
    "    # hero, non-hero, food, spell, side-facing\n",
    "    [1,0,0,0,0,0,0,0,0,0],\n",
    "    [0,1,0,0,0,0,0,0,0,0],\n",
    "    [0,0,1,0,0,0,0,0,0,0],\n",
    "    [0,0,0,1,0,0,0,0,0,0],\n",
    "    [0,0,0,0,1,0,0,0,0,0],\n",
    "    [0,0,0,0,0,1,0,0,0,0],\n",
    "    [0,0,0,0,0,0,1,0,0,0],\n",
    "    [0,0,0,0,0,0,0,1,0,0],\n",
    "    [0,0,0,0,0,0,0,0,1,0],\n",
    "    [0,0,0,0,0,0,0,0,0,1]\n",
    "]).float().to(device)\n",
    "samples, _ = sample_ddpm_context(ctx.shape[0], ctx)\n",
    "show_images(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UitzRaRUwREv"
   },
   "outputs": [],
   "source": [
    "# mix of defined context\n",
    "ctx = torch.tensor([\n",
    "    [1,0,0,0,0,0,0,0,0,0],\n",
    "    [0.9,0.1,0,0,0,0,0,0,0,0],\n",
    "    [0.8,0.2,0,0,0,0,0,0,0,0],\n",
    "    [0.7,0.3,0,0,0,0,0,0,0,0],\n",
    "    [0.6,0.4,0,0,0,0,0,0,0,0],\n",
    "    [0.5,0.5,0,0,0,0,0,0,0,0],\n",
    "    [0.4,0.6,0,0,0,0,0,0,0,0],\n",
    "    [0.3,0.7,0,0,0,0,0,0,0,0],\n",
    "    [0.2,0.8,0,0,0,0,0,0,0,0],\n",
    "    [0.1,0.9,0,0,0,0,0,0,0,0],\n",
    "    [0,1,0,0,0,0,0,0,0,0],\n",
    "]).float().to(device)\n",
    "\n",
    "samples, _ = sample_ddpm_context(ctx.shape[0], ctx)\n",
    "show_images(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pgp_SNDTwREv"
   },
   "outputs": [],
   "source": [
    "# user defined context\n",
    "ctx = torch.tensor([\n",
    "    # hero, non-hero, food, spell, side-facing\n",
    "    [1,0,0,0,0,0,0,0,0,0],\n",
    "    [0,1,0,0,0,0,0,0,0,0],\n",
    "    [0,0,1,0,0,0,0,0,0,0],\n",
    "    [0,0,0,1,0,0,0,0,0,0],\n",
    "    [0,0,0,0,1,0,0,0,0,0],\n",
    "    [0,0,0,0,0,1,0,0,0,0],\n",
    "    [0,0,0,0,0,0,1,0,0,0],\n",
    "    [0,0,0,0,0,0,0,1,0,0],\n",
    "    [0,0,0,0,0,0,0,0,1,0],\n",
    "    [0,0,0,0,0,0,0,0,0,1]\n",
    "]).float().to(device)\n",
    "samples, _ = sample_ddim_context(ctx.shape[0], ctx)\n",
    "show_images(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jOUjjuZkwREv"
   },
   "outputs": [],
   "source": [
    "# mix of defined context\n",
    "ctx = torch.tensor([\n",
    "    [1,0,0,0,0,0,0,0,0,0],\n",
    "    [0.9,0.1,0,0,0,0,0,0,0,0],\n",
    "    [0.8,0.2,0,0,0,0,0,0,0,0],\n",
    "    [0.7,0.3,0,0,0,0,0,0,0,0],\n",
    "    [0.6,0.4,0,0,0,0,0,0,0,0],\n",
    "    [0.5,0.5,0,0,0,0,0,0,0,0],\n",
    "    [0.4,0.6,0,0,0,0,0,0,0,0],\n",
    "    [0.3,0.7,0,0,0,0,0,0,0,0],\n",
    "    [0.2,0.8,0,0,0,0,0,0,0,0],\n",
    "    [0.1,0.9,0,0,0,0,0,0,0,0],\n",
    "    [0,1,0,0,0,0,0,0,0,0],\n",
    "]).float().to(device)\n",
    "\n",
    "samples, _ = sample_ddim_context(ctx.shape[0], ctx)\n",
    "show_images(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p0kZVb_0wREv"
   },
   "outputs": [],
   "source": [
    "loss_values_1 = torch.tensor(loss_values_1).cpu()\n",
    "plt.plot(np.array(loss_values_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Iz7xOoOwREw"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "nn_model.train()\n",
    "guide_w = 0.3\n",
    "loss_values_2=[]\n",
    "for ep in range(n_epoch):\n",
    "    optim.param_groups[0]['lr'] = lrate*(1-ep/n_epoch)\n",
    "    pbar = tqdm(dataloader, mininterval = 2)\n",
    "    for x, c in pbar:\n",
    "        optim.zero_grad()\n",
    "        x=x.to(device)\n",
    "        c=c.to(device)\n",
    "        c=make_encoding(c)\n",
    "    #print(c.shape)\n",
    "        context_mask = torch.bernoulli(torch.zeros(c.shape[0]) + 0.9).to(device)\n",
    "\n",
    "    #print(c.shape)\n",
    "        noise = torch.randn_like(x)\n",
    "        t = torch.randint(1, timesteps + 1, (x.shape[0],))\n",
    "        x_pert = perturb_input(x, t, noise).to(device)\n",
    "        pred_noise_2 = nn_model(x_pert, t/timesteps, c=None)\n",
    "        pred_noise_1 = nn_model(x_pert, t/timesteps, c)\n",
    "        pred_noise_final = eps = (1+guide_w)*pred_noise_1 - guide_w*pred_noise_2\n",
    "        loss = F.mse_loss(pred_noise_final, noise)\n",
    "        loss_values_2.append(loss)\n",
    "        loss.backward()\n",
    "        optim.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mISvXGZbwREw"
   },
   "outputs": [],
   "source": [
    "loss_values_2 = torch.tensor(loss_values_2).cpu()\n",
    "plt.plot(np.array(loss_values_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BeVMrpOkwREw"
   },
   "outputs": [],
   "source": [
    "# user defined context\n",
    "ctx = torch.tensor([\n",
    "    # hero, non-hero, food, spell, side-facing\n",
    "    [1,0,0,0,0,0,0,0,0,0],\n",
    "    [0,1,0,0,0,0,0,0,0,0],\n",
    "    [0,0,1,0,0,0,0,0,0,0],\n",
    "    [0,0,0,1,0,0,0,0,0,0],\n",
    "    [0,0,0,0,1,0,0,0,0,0],\n",
    "    [0,0,0,0,0,1,0,0,0,0],\n",
    "    [0,0,0,0,0,0,1,0,0,0],\n",
    "    [0,0,0,0,0,0,0,1,0,0],\n",
    "    [0,0,0,0,0,0,0,0,1,0],\n",
    "    [0,0,0,0,0,0,0,0,0,1]\n",
    "]).float().to(device)\n",
    "samples, _ = sample_ddpm_context(ctx.shape[0], ctx)\n",
    "show_images(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8MiuMv8XWbMG"
   },
   "outputs": [],
   "source": [
    "# mix of defined context\n",
    "ctx = torch.tensor([\n",
    "    [1,0,0,0,0,0,0,0,0,0],\n",
    "    [0.9,0.1,0,0,0,0,0,0,0,0],\n",
    "    [0.8,0.2,0,0,0,0,0,0,0,0],\n",
    "    [0.7,0.3,0,0,0,0,0,0,0,0],\n",
    "    [0.6,0.4,0,0,0,0,0,0,0,0],\n",
    "    [0.5,0.5,0,0,0,0,0,0,0,0],\n",
    "    [0.4,0.6,0,0,0,0,0,0,0,0],\n",
    "    [0.3,0.7,0,0,0,0,0,0,0,0],\n",
    "    [0.2,0.8,0,0,0,0,0,0,0,0],\n",
    "    [0.1,0.9,0,0,0,0,0,0,0,0],\n",
    "    [0,1,0,0,0,0,0,0,0,0],\n",
    "]).float().to(device)\n",
    "\n",
    "samples, _ = sample_ddpm_context(ctx.shape[0], ctx)\n",
    "show_images(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_omqw-bfwREw"
   },
   "outputs": [],
   "source": [
    "# user defined context\n",
    "ctx = torch.tensor([\n",
    "    # hero, non-hero, food, spell, side-facing\n",
    "    [1,0,0,0,0,0,0,0,0,0],\n",
    "    [0,1,0,0,0,0,0,0,0,0],\n",
    "    [0,0,1,0,0,0,0,0,0,0],\n",
    "    [0,0,0,1,0,0,0,0,0,0],\n",
    "    [0,0,0,0,1,0,0,0,0,0],\n",
    "    [0,0,0,0,0,1,0,0,0,0],\n",
    "    [0,0,0,0,0,0,1,0,0,0],\n",
    "    [0,0,0,0,0,0,0,1,0,0],\n",
    "    [0,0,0,0,0,0,0,0,1,0],\n",
    "    [0,0,0,0,0,0,0,0,0,1]\n",
    "]).float().to(device)\n",
    "samples, _ = sample_ddim_context(ctx.shape[0], ctx)\n",
    "show_images(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OmbAVlJKwREw"
   },
   "outputs": [],
   "source": [
    "# mix of defined context\n",
    "ctx = torch.tensor([\n",
    "    [1,0,0,0,0,0,0,0,0,0],\n",
    "    [0.9,0.1,0,0,0,0,0,0,0,0],\n",
    "    [0.8,0.2,0,0,0,0,0,0,0,0],\n",
    "    [0.7,0.3,0,0,0,0,0,0,0,0],\n",
    "    [0.6,0.4,0,0,0,0,0,0,0,0],\n",
    "    [0.5,0.5,0,0,0,0,0,0,0,0],\n",
    "    [0.4,0.6,0,0,0,0,0,0,0,0],\n",
    "    [0.3,0.7,0,0,0,0,0,0,0,0],\n",
    "    [0.2,0.8,0,0,0,0,0,0,0,0],\n",
    "    [0.1,0.9,0,0,0,0,0,0,0,0],\n",
    "    [0,1,0,0,0,0,0,0,0,0],\n",
    "]).float().to(device)\n",
    "\n",
    "samples, _ = sample_ddim_context(ctx.shape[0], ctx)\n",
    "show_images(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XkNAgR4cwREw"
   },
   "outputs": [],
   "source": [
    "# mix of defined context\n",
    "ctx = torch.tensor([\n",
    "    [1,0,0,0,0,0,0,0,0,0],\n",
    "    [0.9,0.1,0,0,0,0,0,0,0,0],\n",
    "    [0.8,0.2,0,0,0,0,0,0,0,0],\n",
    "    [0.7,0.3,0,0,0,0,0,0,0,0],\n",
    "    [0.6,0.4,0,0,0,0,0,0,0,0],\n",
    "    [0.5,0.5,0,0,0,0,0,0,0,0],\n",
    "    [0.4,0.6,0,0,0,0,0,0,0,0],\n",
    "    [0.3,0.7,0,0,0,0,0,0,0,0],\n",
    "    [0.2,0.8,0,0,0,0,0,0,0,0],\n",
    "    [0.1,0.9,0,0,0,0,0,0,0,0],\n",
    "    [0,1,0,0,0,0,0,0,0,0],\n",
    "]).float().to(device)\n",
    "\n",
    "samples, _ = sample_ddim_context(ctx.shape[0], None)\n",
    "show_images(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R6dLWSedWWQ1"
   },
   "outputs": [],
   "source": [
    "# visualize samples with randomly selected context\n",
    "plt.clf()\n",
    "ctx = torch.tensor([\n",
    "    # hero, non-hero, food, spell, side-facing\n",
    "    [1,0,0,0,0,0,0,0,0,0],\n",
    "    [0,1,0,0,0,0,0,0,0,0],\n",
    "    [0,0,1,0,0,0,0,0,0,0],\n",
    "    [0,0,0,1,0,0,0,0,0,0],\n",
    "    [0,0,0,0,1,0,0,0,0,0],\n",
    "    [0,0,0,0,0,1,0,0,0,0],\n",
    "    [0,0,0,0,0,0,1,0,0,0],\n",
    "    [0,0,0,0,0,0,0,1,0,0],\n",
    "    [0,0,0,0,0,0,0,0,1,0],\n",
    "    [0,0,0,0,0,0,0,0,0,1]\n",
    "]).float().to(device)\n",
    "samples, _ = sample_ddpm_context(ctx.shape[0], ctx)\n",
    "#animation_ddpm_context = plot_sample(intermediate,32,4,save_dir, \"ani_run\", None, save=False)\n",
    "#HTML(animation_ddpm_context.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OUT-2dDCWfrg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oXgPRxJcbR9f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hvkx83T2wRE0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rCF3-HVbwRE0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BTRY_ULvwRE0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "40xtujMEwRE0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kWNOwK3YwRE0"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
